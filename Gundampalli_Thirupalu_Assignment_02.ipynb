{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gtakhil95/Akhil_INFO5731_Fall2024/blob/main/Gundampalli_Thirupalu_Assignment_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ryk8D1Q4Wsrp"
      },
      "source": [
        "# **INFO5731 Assignment 2**\n",
        "\n",
        "In this assignment, you will work on gathering text data from an open data source via web scraping or API. Following this, you will need to clean the text data and perform syntactic analysis on the data. Follow the instructions carefully and design well-structured Python programs to address each question.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "* **Make sure to submit the cleaned data CSV in the comment section - 10 points**\n",
        "\n",
        "**Total points**: 100\n",
        "\n",
        "**Deadline**: Tuesday, at 11:59 PM.\n",
        "\n",
        "**Late Submission will have a penalty of 10% reduction for each day after the deadline.**\n",
        "\n",
        "**Please check that the link you submitted can be opened and points to the correct assignment.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkzR8cFAyGik"
      },
      "source": [
        "# Question 1 (40 points)\n",
        "\n",
        "Write a python program to collect text data from **either of the following sources** and save the data into a **csv file:**\n",
        "\n",
        "(1) Collect all the customer reviews of a product (you can choose any porduct) on amazon. [atleast 1000 reviews]\n",
        "\n",
        "(2) Collect the top 1000 User Reviews of a movie recently in 2023 or 2024 (you can choose any movie) from IMDB. [If one movie doesn't have sufficient reviews, collect reviews of atleast 2 or 3 movies]\n",
        "\n",
        "(3) Collect all the reviews of the top 1000 most popular software from G2 or Capterra.\n",
        "\n",
        "(4) Collect the **abstracts** of the top 10000 research papers by using the query \"machine learning\", \"data science\", \"artifical intelligence\", or \"information extraction\" from Semantic Scholar.\n",
        "\n",
        "(5) Collect all the information of the 904 narrators in the Densho Digital Repository.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install requests pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-BI1fr3nK1Z",
        "outputId": "551e4292-7e79-4618-9ea0-df56764b833f"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Set up your Semantic Scholar API key here\n",
        "API_KEY = 'PxzYpR0ff55xIhVZQrbBsK4zC5AMtHJ1AkEwCiEh'\n",
        "\n",
        "# Headers for requests including your API key\n",
        "headers = {\n",
        "    'x-api-key': API_KEY,\n",
        "    'Accept': 'application/json'\n",
        "}\n",
        "\n",
        "# Define the queries\n",
        "queries = [\"machine learning\", \"data science\", \"artificial intelligence\", \"information extraction\",\"robotics\",\"neural networks\",\"data analyst\",\"data extraction\",\"artificial intelligence\",\"information extraction\"]\n",
        "\n",
        "# Set up the base URL for the Semantic Scholar API\n",
        "BASE_URL = \"https://api.semanticscholar.org/graph/v1/paper/search\"\n",
        "\n",
        "# Function to fetch abstracts from Semantic Scholar\n",
        "def fetch_papers(query, limit, offset):\n",
        "    params = {\n",
        "        'query': query,\n",
        "        'limit': limit,\n",
        "        'offset': offset,\n",
        "        'fields': 'title,abstract'\n",
        "    }\n",
        "    response = requests.get(BASE_URL, headers=headers, params=params)\n",
        "\n",
        "    # Raise an exception if the request was not successful\n",
        "    if response.status_code != 200:\n",
        "        raise Exception(f\"Error fetching data: {response.status_code}\")\n",
        "\n",
        "    # Return the JSON response\n",
        "    return response.json()\n",
        "\n",
        "# Function to collect all abstracts\n",
        "def collect_abstracts(queries, total_papers=10000):\n",
        "    all_papers = []\n",
        "    papers_per_query = total_papers // len(queries)  # Split papers equally across queries\n",
        "\n",
        "    for query in queries:\n",
        "        offset = 0\n",
        "        while len(all_papers) < total_papers:\n",
        "            try:\n",
        "                print(f\"Fetching papers for query: {query}, Offset: {offset}\")\n",
        "\n",
        "                # Fetch the next batch of papers\n",
        "                result = fetch_papers(query, limit=100, offset=offset)\n",
        "\n",
        "                if 'data' not in result or len(result['data']) == 0:\n",
        "                    print(f\"No more papers found for query: {query}\")\n",
        "                    break\n",
        "\n",
        "                # Extract relevant paper data (title, abstract)\n",
        "                for paper in result['data']:\n",
        "                    title = paper.get('title', 'No title available')\n",
        "                    abstract = paper.get('abstract', 'No abstract available')\n",
        "                    all_papers.append([title, abstract])\n",
        "\n",
        "                    if len(all_papers) >= total_papers:\n",
        "                        break\n",
        "\n",
        "                # Increment offset to get the next batch of papers\n",
        "                offset += 100\n",
        "                time.sleep(1)  # Add delay to avoid rate limits\n",
        "            except Exception as e:\n",
        "                print(f\"Error fetching papers: {e}\")\n",
        "                break\n",
        "\n",
        "    return all_papers\n",
        "\n",
        "# Collect the top 10,000 abstracts\n",
        "abstracts_data = collect_abstracts(queries, total_papers=10000)\n",
        "\n",
        "# Save the abstracts data to a CSV file\n",
        "df = pd.DataFrame(abstracts_data, columns=['Title', 'Abstract'])\n",
        "df.to_csv('top_10000_abstracts.csv', index=False)\n",
        "\n",
        "print(\"Abstracts saved to 'top_10000_abstracts.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3JvlZLcpNeI",
        "outputId": "2992235c-4348-4eb9-be59-7e6ab291edd0"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching papers for query: machine learning, Offset: 0\n",
            "Fetching papers for query: machine learning, Offset: 100\n",
            "Fetching papers for query: machine learning, Offset: 200\n",
            "Fetching papers for query: machine learning, Offset: 300\n",
            "Fetching papers for query: machine learning, Offset: 400\n",
            "Fetching papers for query: machine learning, Offset: 500\n",
            "Fetching papers for query: machine learning, Offset: 600\n",
            "Fetching papers for query: machine learning, Offset: 700\n",
            "Fetching papers for query: machine learning, Offset: 800\n",
            "Fetching papers for query: machine learning, Offset: 900\n",
            "Fetching papers for query: machine learning, Offset: 1000\n",
            "Error fetching papers: Error fetching data: 400\n",
            "Fetching papers for query: data science, Offset: 0\n",
            "Fetching papers for query: data science, Offset: 100\n",
            "Fetching papers for query: data science, Offset: 200\n",
            "Fetching papers for query: data science, Offset: 300\n",
            "Fetching papers for query: data science, Offset: 400\n",
            "Fetching papers for query: data science, Offset: 500\n",
            "Fetching papers for query: data science, Offset: 600\n",
            "Fetching papers for query: data science, Offset: 700\n",
            "Fetching papers for query: data science, Offset: 800\n",
            "Fetching papers for query: data science, Offset: 900\n",
            "Fetching papers for query: data science, Offset: 1000\n",
            "Error fetching papers: Error fetching data: 400\n",
            "Fetching papers for query: artificial intelligence, Offset: 0\n",
            "Fetching papers for query: artificial intelligence, Offset: 100\n",
            "Fetching papers for query: artificial intelligence, Offset: 200\n",
            "Fetching papers for query: artificial intelligence, Offset: 300\n",
            "Fetching papers for query: artificial intelligence, Offset: 400\n",
            "Fetching papers for query: artificial intelligence, Offset: 500\n",
            "Fetching papers for query: artificial intelligence, Offset: 600\n",
            "Fetching papers for query: artificial intelligence, Offset: 700\n",
            "Fetching papers for query: artificial intelligence, Offset: 800\n",
            "Fetching papers for query: artificial intelligence, Offset: 900\n",
            "Fetching papers for query: artificial intelligence, Offset: 1000\n",
            "Error fetching papers: Error fetching data: 400\n",
            "Fetching papers for query: information extraction, Offset: 0\n",
            "Fetching papers for query: information extraction, Offset: 100\n",
            "Fetching papers for query: information extraction, Offset: 200\n",
            "Fetching papers for query: information extraction, Offset: 300\n",
            "Fetching papers for query: information extraction, Offset: 400\n",
            "Fetching papers for query: information extraction, Offset: 500\n",
            "Fetching papers for query: information extraction, Offset: 600\n",
            "Fetching papers for query: information extraction, Offset: 700\n",
            "Fetching papers for query: information extraction, Offset: 800\n",
            "Fetching papers for query: information extraction, Offset: 900\n",
            "Fetching papers for query: information extraction, Offset: 1000\n",
            "Error fetching papers: Error fetching data: 400\n",
            "Fetching papers for query: robotics, Offset: 0\n",
            "Fetching papers for query: robotics, Offset: 100\n",
            "Fetching papers for query: robotics, Offset: 200\n",
            "Fetching papers for query: robotics, Offset: 300\n",
            "Fetching papers for query: robotics, Offset: 400\n",
            "Fetching papers for query: robotics, Offset: 500\n",
            "Fetching papers for query: robotics, Offset: 600\n",
            "Fetching papers for query: robotics, Offset: 700\n",
            "Fetching papers for query: robotics, Offset: 800\n",
            "Fetching papers for query: robotics, Offset: 900\n",
            "Fetching papers for query: robotics, Offset: 1000\n",
            "Error fetching papers: Error fetching data: 400\n",
            "Fetching papers for query: neural networks, Offset: 0\n",
            "Fetching papers for query: neural networks, Offset: 100\n",
            "Fetching papers for query: neural networks, Offset: 200\n",
            "Fetching papers for query: neural networks, Offset: 300\n",
            "Fetching papers for query: neural networks, Offset: 400\n",
            "Fetching papers for query: neural networks, Offset: 500\n",
            "Fetching papers for query: neural networks, Offset: 600\n",
            "Fetching papers for query: neural networks, Offset: 700\n",
            "Fetching papers for query: neural networks, Offset: 800\n",
            "Fetching papers for query: neural networks, Offset: 900\n",
            "Fetching papers for query: neural networks, Offset: 1000\n",
            "Error fetching papers: Error fetching data: 400\n",
            "Fetching papers for query: data analyst, Offset: 0\n",
            "Fetching papers for query: data analyst, Offset: 100\n",
            "Fetching papers for query: data analyst, Offset: 200\n",
            "Fetching papers for query: data analyst, Offset: 300\n",
            "Fetching papers for query: data analyst, Offset: 400\n",
            "Fetching papers for query: data analyst, Offset: 500\n",
            "Fetching papers for query: data analyst, Offset: 600\n",
            "Fetching papers for query: data analyst, Offset: 700\n",
            "Fetching papers for query: data analyst, Offset: 800\n",
            "Fetching papers for query: data analyst, Offset: 900\n",
            "Fetching papers for query: data analyst, Offset: 1000\n",
            "Error fetching papers: Error fetching data: 400\n",
            "Fetching papers for query: data extraction, Offset: 0\n",
            "Fetching papers for query: data extraction, Offset: 100\n",
            "Fetching papers for query: data extraction, Offset: 200\n",
            "Fetching papers for query: data extraction, Offset: 300\n",
            "Fetching papers for query: data extraction, Offset: 400\n",
            "Fetching papers for query: data extraction, Offset: 500\n",
            "Fetching papers for query: data extraction, Offset: 600\n",
            "Fetching papers for query: data extraction, Offset: 700\n",
            "Fetching papers for query: data extraction, Offset: 800\n",
            "Fetching papers for query: data extraction, Offset: 900\n",
            "Fetching papers for query: data extraction, Offset: 1000\n",
            "Error fetching papers: Error fetching data: 400\n",
            "Fetching papers for query: artificial intelligence, Offset: 0\n",
            "Fetching papers for query: artificial intelligence, Offset: 100\n",
            "Fetching papers for query: artificial intelligence, Offset: 200\n",
            "Fetching papers for query: artificial intelligence, Offset: 300\n",
            "Fetching papers for query: artificial intelligence, Offset: 400\n",
            "Fetching papers for query: artificial intelligence, Offset: 500\n",
            "Fetching papers for query: artificial intelligence, Offset: 600\n",
            "Fetching papers for query: artificial intelligence, Offset: 700\n",
            "Fetching papers for query: artificial intelligence, Offset: 800\n",
            "Fetching papers for query: artificial intelligence, Offset: 900\n",
            "Fetching papers for query: artificial intelligence, Offset: 1000\n",
            "Error fetching papers: Error fetching data: 400\n",
            "Fetching papers for query: information extraction, Offset: 0\n",
            "Fetching papers for query: information extraction, Offset: 100\n",
            "Fetching papers for query: information extraction, Offset: 200\n",
            "Fetching papers for query: information extraction, Offset: 300\n",
            "Fetching papers for query: information extraction, Offset: 400\n",
            "Fetching papers for query: information extraction, Offset: 500\n",
            "Fetching papers for query: information extraction, Offset: 600\n",
            "Fetching papers for query: information extraction, Offset: 700\n",
            "Fetching papers for query: information extraction, Offset: 800\n",
            "Fetching papers for query: information extraction, Offset: 900\n",
            "Abstracts saved to 'top_10000_abstracts.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90_NR8c5XGWc"
      },
      "source": [
        "# Question 2 (30 points)\n",
        "\n",
        "Write a python program to **clean the text data** you collected in the previous question and save the clean data in a new column in the csv file. The data cleaning steps include: [Code and output is required for each part]\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the stopwords list.\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming.\n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "5QX6bJjGWXY9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25010919-e159-4516-ca67-0c5f57e57c75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "# Write code for each of the sub parts with proper comments.\n",
        "!pip install nltk pandas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download the necessary NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load the dataset (assumes you have the CSV created from the previous step)\n",
        "df = pd.read_csv('top_10000_abstracts.csv')\n",
        "\n",
        "# Preview the data\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwiMDLYhtiN6",
        "outputId": "06f9e5c2-52a6-442a-965b-922d551a1547"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                               Title  \\\n",
            "0  Fashion-MNIST: a Novel Image Dataset for Bench...   \n",
            "1  TensorFlow: A system for large-scale machine l...   \n",
            "2  TensorFlow: Large-Scale Machine Learning on He...   \n",
            "3  Stop explaining black box machine learning mod...   \n",
            "4  Convolutional LSTM Network: A Machine Learning...   \n",
            "\n",
            "                                            Abstract  \n",
            "0  We present Fashion-MNIST, a new dataset compri...  \n",
            "1  TensorFlow is a machine learning system that o...  \n",
            "2  TensorFlow is an interface for expressing mach...  \n",
            "3                                                NaN  \n",
            "4  The goal of precipitation nowcasting is to pre...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_special_characters(text):\n",
        "    # Check if the input is a string before applying regex\n",
        "    if isinstance(text, str):\n",
        "        return re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    else:\n",
        "        return text  # Return the original value if it's not a string\n",
        "\n",
        "# Apply this to the 'Abstract' column\n",
        "df['cleaned_abstract'] = df['Abstract'].apply(remove_special_characters)\n",
        "\n",
        "print(df[['Abstract', 'cleaned_abstract']].head())\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_gPJ8Dpt3FQ",
        "outputId": "430fad3c-a219-440f-8835-b358af05a62a"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                            Abstract  \\\n",
            "0  We present Fashion-MNIST, a new dataset compri...   \n",
            "1  TensorFlow is a machine learning system that o...   \n",
            "2  TensorFlow is an interface for expressing mach...   \n",
            "3                                                NaN   \n",
            "4  The goal of precipitation nowcasting is to pre...   \n",
            "\n",
            "                                    cleaned_abstract  \n",
            "0  We present FashionMNIST a new dataset comprisi...  \n",
            "1  TensorFlow is a machine learning system that o...  \n",
            "2  TensorFlow is an interface for expressing mach...  \n",
            "3                                                NaN  \n",
            "4  The goal of precipitation nowcasting is to pre...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_numbers(text):\n",
        "    # Check if the input is a string before applying regex\n",
        "    if isinstance(text, str):\n",
        "        return re.sub(r'\\d+', '', text)\n",
        "    else:\n",
        "        return str(text)  # Convert non-string values to string\n",
        "\n",
        "# Apply this to the 'cleaned_abstract' column\n",
        "df['cleaned_abstract'] = df['cleaned_abstract'].apply(remove_numbers)\n",
        "\n",
        "print(df[['cleaned_abstract']].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmFEkH3oubUN",
        "outputId": "b49f72b5-5034-416d-9a29-03517b393410"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                    cleaned_abstract\n",
            "0  We present FashionMNIST a new dataset comprisi...\n",
            "1  TensorFlow is a machine learning system that o...\n",
            "2  TensorFlow is an interface for expressing mach...\n",
            "3                                                nan\n",
            "4  The goal of precipitation nowcasting is to pre...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    words = text.split()\n",
        "    return ' '.join([word for word in words if word.lower() not in stop_words])\n",
        "\n",
        "# Apply this to the 'cleaned_abstract' column\n",
        "df['cleaned_abstract'] = df['cleaned_abstract'].apply(remove_stopwords)\n",
        "\n",
        "print(df[['cleaned_abstract']].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ic5d_e5numU8",
        "outputId": "1a4d19a7-5dc8-41cb-940b-7c016cb26486"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                    cleaned_abstract\n",
            "0  present FashionMNIST new dataset comprising x ...\n",
            "1  TensorFlow machine learning system operates la...\n",
            "2  TensorFlow interface expressing machine learni...\n",
            "3                                                nan\n",
            "4  goal precipitation nowcasting predict future r...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def to_lowercase(text):\n",
        "    return text.lower()\n",
        "\n",
        "# Apply this to the 'cleaned_abstract' column\n",
        "df['cleaned_abstract'] = df['cleaned_abstract'].apply(to_lowercase)\n",
        "\n",
        "print(df[['cleaned_abstract']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8VxEQNbuqyl",
        "outputId": "0414b04a-3233-4b5f-cda2-be6b0c9e7d82"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                    cleaned_abstract\n",
            "0  present fashionmnist new dataset comprising x ...\n",
            "1  tensorflow machine learning system operates la...\n",
            "2  tensorflow interface expressing machine learni...\n",
            "3                                                nan\n",
            "4  goal precipitation nowcasting predict future r...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ps = PorterStemmer()\n",
        "\n",
        "def stem_words(text):\n",
        "    words = text.split()\n",
        "    return ' '.join([ps.stem(word) for word in words])\n",
        "\n",
        "# Apply this to the 'cleaned_abstract' column\n",
        "df['cleaned_abstract'] = df['cleaned_abstract'].apply(stem_words)\n",
        "\n",
        "print(df[['cleaned_abstract']].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "daDK7XRQuySQ",
        "outputId": "697d5e63-ef2c-4c09-87c4-e0f125738ec9"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                    cleaned_abstract\n",
            "0  present fashionmnist new dataset compris x gra...\n",
            "1  tensorflow machin learn system oper larg scale...\n",
            "2  tensorflow interfac express machin learn algor...\n",
            "3                                                nan\n",
            "4  goal precipit nowcast predict futur rainfal in...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatize_words(text):\n",
        "    words = text.split()\n",
        "    return ' '.join([lemmatizer.lemmatize(word) for word in words])\n",
        "\n",
        "# Apply this to the 'cleaned_abstract' column\n",
        "df['cleaned_abstract_lemmatized'] = df['cleaned_abstract'].apply(lemmatize_words)\n",
        "\n",
        "print(df[['cleaned_abstract', 'cleaned_abstract_lemmatized']].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FvuZcHSWu6X0",
        "outputId": "d8ceffc0-a5a7-4c48-e217-1702e008f22c"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                    cleaned_abstract  \\\n",
            "0  present fashionmnist new dataset compris x gra...   \n",
            "1  tensorflow machin learn system oper larg scale...   \n",
            "2  tensorflow interfac express machin learn algor...   \n",
            "3                                                nan   \n",
            "4  goal precipit nowcast predict futur rainfal in...   \n",
            "\n",
            "                         cleaned_abstract_lemmatized  \n",
            "0  present fashionmnist new dataset compris x gra...  \n",
            "1  tensorflow machin learn system oper larg scale...  \n",
            "2  tensorflow interfac express machin learn algor...  \n",
            "3                                                nan  \n",
            "4  goal precipit nowcast predict futur rainfal in...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the cleaned data to a new CSV file\n",
        "df.to_csv('cleaned_top_10000_abstracts.csv', index=False)\n",
        "print(\"Cleaned data saved to cleaned_top_10000_abstracts.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5TuFRvyvExp",
        "outputId": "42bd0398-c9c9-45fb-be3b-2726c7ad96e1"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned data saved to cleaned_top_10000_abstracts.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F_PZdH9Sh49"
      },
      "source": [
        "# Question 3 (30 points)\n",
        "\n",
        "Write a python program to **conduct syntax and structure analysis of the clean text** you just saved above. The syntax and structure analysis includes:\n",
        "\n",
        "(1) **Parts of Speech (POS) Tagging:** Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) **Constituency Parsing and Dependency Parsing:** print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) **Named Entity Recognition:** Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "Y0oOSlsOS0cq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83a9529b-ad10-4203-95bd-7f0b615c10ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.6)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.8.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.8.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "# Your code here\n",
        "!pip install spacy nltk pandas\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import spacy\n",
        "from collections import Counter\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import treebank\n",
        "\n",
        "# Load the cleaned data\n",
        "df = pd.read_csv('cleaned_top_10000_abstracts.csv')\n",
        "\n",
        "# Load spaCy's English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Preview the data\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWaux6texCZb",
        "outputId": "7686ac9e-684f-49f8-9098-e8f69424a48d"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                               Title  \\\n",
            "0  Fashion-MNIST: a Novel Image Dataset for Bench...   \n",
            "1  TensorFlow: A system for large-scale machine l...   \n",
            "2  TensorFlow: Large-Scale Machine Learning on He...   \n",
            "3  Stop explaining black box machine learning mod...   \n",
            "4  Convolutional LSTM Network: A Machine Learning...   \n",
            "\n",
            "                                            Abstract  \\\n",
            "0  We present Fashion-MNIST, a new dataset compri...   \n",
            "1  TensorFlow is a machine learning system that o...   \n",
            "2  TensorFlow is an interface for expressing mach...   \n",
            "3                                                NaN   \n",
            "4  The goal of precipitation nowcasting is to pre...   \n",
            "\n",
            "                                    cleaned_abstract  \\\n",
            "0  present fashionmnist new dataset compris x gra...   \n",
            "1  tensorflow machin learn system oper larg scale...   \n",
            "2  tensorflow interfac express machin learn algor...   \n",
            "3                                                NaN   \n",
            "4  goal precipit nowcast predict futur rainfal in...   \n",
            "\n",
            "                         cleaned_abstract_lemmatized  \n",
            "0  present fashionmnist new dataset compris x gra...  \n",
            "1  tensorflow machin learn system oper larg scale...  \n",
            "2  tensorflow interfac express machin learn algor...  \n",
            "3                                                NaN  \n",
            "4  goal precipit nowcast predict futur rainfal in...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to perform POS tagging\n",
        "def pos_tagging(text):\n",
        "    # Convert to string to handle non-string values\n",
        "    text = str(text)  # This line ensures 'text' is always a string\n",
        "    tokens = word_tokenize(text)\n",
        "    return pos_tag(tokens)\n",
        "\n",
        "# Apply POS tagging to each abstract\n",
        "df['pos_tags'] = df['cleaned_abstract_lemmatized'].apply(pos_tagging)\n",
        "\n",
        "# Function to count specific POS (Nouns, Verbs, Adjectives, Adverbs)\n",
        "def count_pos_tags(pos_tags):\n",
        "    pos_counts = Counter(tag for word, tag in pos_tags)\n",
        "    noun_count = sum([pos_counts[tag] for tag in ['NN', 'NNS', 'NNP', 'NNPS']])\n",
        "    verb_count = sum([pos_counts[tag] for tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']])\n",
        "    adj_count = sum([pos_counts[tag] for tag in ['JJ', 'JJR', 'JJS']])\n",
        "    adv_count = sum([pos_counts[tag] for tag in ['RB', 'RBR', 'RBS']])\n",
        "\n",
        "    return noun_count, verb_count, adj_count, adv_count\n",
        "\n",
        "# Apply the POS counting function to the DataFrame\n",
        "df['noun_count'], df['verb_count'], df['adj_count'], df['adv_count'] = zip(*df['pos_tags'].apply(count_pos_tags))\n",
        "\n",
        "# Output POS counts for the first few rows\n",
        "print(df[['cleaned_abstract_lemmatized', 'noun_count', 'verb_count', 'adj_count', 'adv_count']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_yJ3nTkIxQm5",
        "outputId": "d6ad486f-94a5-4fd2-9485-ff7e5d76cfea"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                         cleaned_abstract_lemmatized  noun_count  verb_count  \\\n",
            "0  present fashionmnist new dataset compris x gra...          27           6   \n",
            "1  tensorflow machin learn system oper larg scale...          70          10   \n",
            "2  tensorflow interfac express machin learn algor...          70           8   \n",
            "3                                                NaN           1           0   \n",
            "4  goal precipit nowcast predict futur rainfal in...          57           4   \n",
            "\n",
            "   adj_count  adv_count  \n",
            "0         12          0  \n",
            "1         17          3  \n",
            "2         19          0  \n",
            "3          0          0  \n",
            "4         11          2  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import Tree\n",
        "\n",
        "# Using an example sentence for constituency parsing\n",
        "example_sentence = \"Artificial intelligence is transforming many industries.\"\n",
        "\n",
        "# Tokenize and POS tag the example sentence\n",
        "example_tokens = nltk.word_tokenize(example_sentence)\n",
        "example_pos_tags = nltk.pos_tag(example_tokens)\n",
        "\n",
        "# Use a predefined treebank grammar to generate a constituency parse tree (a simplified example)\n",
        "# The original grammar is extended to include a rule for handling the period ('.').\n",
        "# The period is defined as a terminal symbol using single quotes.\n",
        "\n",
        "# Removed comments and placed period in quotes to treat as terminal\n",
        "grammar = nltk.CFG.fromstring(\"\"\"\n",
        "  S -> NP VP '.'\n",
        "  VP -> V NP | V NP PP\n",
        "  PP -> P NP\n",
        "  V -> \"is\" | \"transforming\"\n",
        "  NP -> \"Artificial\" \"intelligence\" | \"many\" \"industries\"\n",
        "  P -> \"in\"\n",
        "\"\"\")\n",
        "parser = nltk.ChartParser(grammar)\n",
        "\n",
        "# Print the constituency parsing tree\n",
        "for tree in parser.parse(example_tokens):\n",
        "    tree.pretty_print()"
      ],
      "metadata": {
        "id": "lcT_gh8nyBel"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to perform dependency parsing\n",
        "def dependency_parsing(text):\n",
        "    doc = nlp(text)\n",
        "    for token in doc:\n",
        "        print(f\"{token.text} -> {token.dep_} -> {token.head.text}\")\n",
        "\n",
        "# Example of dependency parsing for one abstract\n",
        "example_text = df['cleaned_abstract_lemmatized'].iloc[0]\n",
        "print(\"Dependency Parsing for the first abstract:\")\n",
        "dependency_parsing(example_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2EdJYySzVny",
        "outputId": "370c9e09-7941-43eb-b67e-bd635f1718a5"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dependency Parsing for the first abstract:\n",
            "present -> amod -> product\n",
            "fashionmnist -> amod -> product\n",
            "new -> amod -> dataset\n",
            "dataset -> compound -> product\n",
            "compris -> nmod -> product\n",
            "x -> punct -> product\n",
            "grayscal -> amod -> imag\n",
            "imag -> amod -> product\n",
            "fashion -> compound -> product\n",
            "product -> nsubj -> set\n",
            "categori -> aux -> set\n",
            "imag -> acl -> categori\n",
            "per -> prep -> imag\n",
            "categori -> compound -> train\n",
            "train -> nsubj -> set\n",
            "set -> ROOT -> set\n",
            "imag -> amod -> test\n",
            "test -> dobj -> set\n",
            "set -> dep -> set\n",
            "imag -> amod -> fashionmnist\n",
            "fashionmnist -> nsubj -> intend\n",
            "intend -> conj -> set\n",
            "serv -> nmod -> origin\n",
            "direct -> amod -> origin\n",
            "dropin -> nmod -> origin\n",
            "replac -> compound -> origin\n",
            "origin -> compound -> mnist\n",
            "mnist -> compound -> machin\n",
            "dataset -> compound -> benchmark\n",
            "benchmark -> compound -> machin\n",
            "machin -> dobj -> intend\n",
            "learn -> npadvmod -> set\n",
            "algorithm -> compound -> share\n",
            "share -> ccomp -> learn\n",
            "imag -> amod -> format\n",
            "size -> compound -> format\n",
            "data -> compound -> format\n",
            "format -> compound -> test\n",
            "structur -> compound -> test\n",
            "train -> compound -> test\n",
            "test -> nsubj -> split\n",
            "split -> compound -> url\n",
            "dataset -> compound -> url\n",
            "freeli -> amod -> http\n",
            "avail -> compound -> http\n",
            "http -> compound -> url\n",
            "url -> dobj -> set\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract named entities using spaCy\n",
        "def named_entity_recognition(text):\n",
        "    # Convert to string to handle non-string values, including NaNs and floats\n",
        "    text = str(text) if not isinstance(text, str) else text\n",
        "    doc = nlp(text)\n",
        "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "    return entities\n",
        "\n",
        "# Apply named entity recognition to the abstracts\n",
        "df['entities'] = df['cleaned_abstract_lemmatized'].apply(named_entity_recognition)\n",
        "\n",
        "# Count each type of named entity (e.g., PERSON, ORG, GPE, DATE, PRODUCT)\n",
        "def count_entities(entities):\n",
        "    entity_counter = Counter([ent[1] for ent in entities])\n",
        "    return entity_counter\n",
        "\n",
        "df['entity_counts'] = df['entities'].apply(count_entities)\n",
        "\n",
        "# Output the named entities and their counts for the first few rows\n",
        "print(df[['cleaned_abstract_lemmatized', 'entities', 'entity_counts']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJBrzr6_zfu3",
        "outputId": "1e1f541c-31b5-4a56-cfe5-54d21b800c35"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                         cleaned_abstract_lemmatized  \\\n",
            "0  present fashionmnist new dataset compris x gra...   \n",
            "1  tensorflow machin learn system oper larg scale...   \n",
            "2  tensorflow interfac express machin learn algor...   \n",
            "3                                                NaN   \n",
            "4  goal precipit nowcast predict futur rainfal in...   \n",
            "\n",
            "                                            entities  \\\n",
            "0                [(compris, PERSON), (freeli, NORP)]   \n",
            "1  [(heterogen, ORG), (flexibl, DATE), (varieti a...   \n",
            "2  [(chang wide varieti heterogen system rang mob...   \n",
            "3                                                 []   \n",
            "4        [(fulli, GPE), (lstm fclstm convolut, ORG)]   \n",
            "\n",
            "                             entity_counts  \n",
            "0                 {'PERSON': 1, 'NORP': 1}  \n",
            "1       {'ORG': 1, 'DATE': 1, 'PERSON': 1}  \n",
            "2  {'PERSON': 1, 'CARDINAL': 2, 'DATE': 1}  \n",
            "3                                       {}  \n",
            "4                     {'GPE': 1, 'ORG': 1}  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the results into a new CSV file\n",
        "df.to_csv('syntax_structure_analysis.csv', index=False)\n",
        "\n",
        "print(\"Results saved to syntax_structure_analysis.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJcXsA4z1gAI",
        "outputId": "5757d358-6605-4906-b55d-7e9c11e8d84e"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to syntax_structure_analysis.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Comment**\n",
        "Make sure to submit the cleaned data CSV in the comment section - 10 points"
      ],
      "metadata": {
        "id": "CXNn1lEVbMsv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'I have submitted 2 CSV files in the comment section- saved \"cleaned_top_10000_abstracts.csv\" for Question 2 and \"syntax_structure_analysis.csv\" for Question 3'"
      ],
      "metadata": {
        "id": "qYRO5Cn8bYwZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "1ea7d988-bae6-4769-caa3-385a2d5d3e86"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I have submitted 2 CSV files in the comment section- saved \"cleaned_top_10000_abstracts.csv\" for Question 2 and \"syntax_structure_analysis.csv\" for Question 3'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question\n",
        "\n",
        "Provide your thoughts on the assignment. What did you find challenging, and what aspects did you enjoy? Your opinion on the provided time to complete the assignment."
      ],
      "metadata": {
        "id": "q8BFCvWp32cf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your response below\n",
        "'The task was a good challenge that called for a combination of real-world NLP, data processing, and software development skills. The practical applications and theoretical depth were both enjoyable to me, however managing the complex nature of several NLP components while retaining output required a lot of work.'"
      ],
      "metadata": {
        "id": "_e557s2w4BpK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "3acf684f-6705-4b96-f069-a1503429b29a"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The task was a good challenge that called for a combination of real-world NLP, data processing, and software development skills. The practical applications and theoretical depth were both enjoyable to me, however managing the complex nature of several NLP components while retaining output required a lot of work.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}