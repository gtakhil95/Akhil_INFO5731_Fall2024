{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gtakhil95/Akhil_INFO5731_Fall2024/blob/main/Gundampalli_Akhil_Exercise_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DymRJbxDBCnf"
      },
      "source": [
        "# **INFO5731 In-class Exercise 2**\n",
        "\n",
        "The purpose of this exercise is to understand users' information needs, and then collect data from different sources for analysis by implementing web scraping using Python.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBKvD6O_TY6e"
      },
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting research question (or practical question or something innovative) you have in mind, what kind of data should be collected to answer the question(s)? Specify the amount of data needed for analysis. Provide detailed steps for collecting and saving the data."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The research question is, \"Do social media trends on road accidents occur annually in the US?\"\n",
        "\n",
        "  1.  Identify Key Data Sources\n",
        "We should focus on gathering data from platforms where people commonly discuss road accidents:\n",
        "    *  Twitter: Posts (tweets) related to road accidents.\n",
        "    *  Facebook: Public posts and groups focused on traffic safety or road incidents.\n",
        "    *  Reddit: Subreddits related to accidents, news, and public safety.\n",
        "    *  Instagram: Posts tagged with keywords related to road accidents.\n",
        "    *  News websites that post on social media: These may also have road accident reports that get shared widely.\n",
        "\n",
        "  2.  Define Relevant Timeframe\n",
        "   To capture annual trends, we need to collect data from a minimum of  3 to 5 years to observe recurring patterns. It would be ideal to have data covering different seasons each year, since road accidents may follow seasonal patterns.\n",
        "\n",
        "  3.  Steps for Data Collection\n",
        "\n",
        "Step 1:  Develop a Data Collection Plan\n",
        "\n",
        "    * Choose platforms: Identify which platforms are most likely to have relevant discussions. We can prioritize Reddit due to their open-access APIs.\n",
        "    *  Select tools: Use tools such as:\n",
        "      *  Social media APIs (Twitter API, Reddit API).\n",
        "      *  Web scraping tools (BeautifulSoup for scraping Instagram posts, or Scrapy for forums).\n",
        "      *  Sentiment analysis tools for processing text (VADER or TextBlob).\n",
        "\n",
        "Step 2:  Keyword Identification and Refinement\n",
        "\n",
        "    * Compile a list of keywords and hashtags commonly associated with road accidents.\n",
        "    * Use a test run to gather a small dataset and refine the keyword list if necessary (remove irrelevant or vague terms).\n",
        "\n",
        "Step 3:  Data Collection\n",
        "\n",
        "    *  API/Tool setup: Create accounts and set up access to social media platformsâ€™ APIs.\n",
        "    *  Gather historical data: If possible, retrieve historical data from social media platforms using keywords or hashtags.\n",
        "    *  Ensure US Focus: Filter posts or mentions to only include those from the US, possibly using geotagged data or language filters (for mentions of US cities or states).\n",
        "    *  Frequency: Set the collection to grab data at regular intervals (e.g., daily, weekly) for continuous monitoring over several years.\n",
        "\n",
        "Step 4:  Data Storage and Organization\n",
        "\n",
        "    *  Store data in a structured format: Save the collected data in formats such as CSV or JSON for easy analysis.\n",
        "\n",
        "  4.  Amount of Data Needed\n",
        "    *  Sample size: We need to aim for at least 10,000 to 100,000 social media posts per year to have a sufficient sample size for analysis, depending on the platform.\n",
        "    *  Length of study: Collect data from 3 to 5 years to detect annual trends\n"
      ],
      "metadata": {
        "id": "qIS49ogBEN8y"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9RqrlwdTfvl"
      },
      "source": [
        "## Question 2 (10 Points)\n",
        "Write Python code to collect a dataset of 1000 samples related to the question discussed in Question 1."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install praw\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrUYUtYqZtRn",
        "outputId": "e40e5938-e324-488f-ad5c-15190cb17439"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: praw in /usr/local/lib/python3.10/dist-packages (7.7.1)\n",
            "Requirement already satisfied: prawcore<3,>=2.1 in /usr/local/lib/python3.10/dist-packages (from praw) (2.4.0)\n",
            "Requirement already satisfied: update-checker>=0.18 in /usr/local/lib/python3.10/dist-packages (from praw) (0.18.0)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.10/dist-packages (from praw) (1.8.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from prawcore<3,>=2.1->praw) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import praw\n",
        "import pandas as pd\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Step 1: Set up Reddit API connection using PRAW\n",
        "reddit = praw.Reddit(client_id='F4rfCMsg7srHMauT3EauVA',\n",
        "                     client_secret='Yj8hwZkknqi_xpWGvB6FgF3EGDLmzQ',\n",
        "                     user_agent='Big_Jaguar_8911')\n",
        "\n",
        "# Step 2: Specify subreddits and keywords\n",
        "subreddits = ['carcrash', 'roadcam', 'cars', 'publicfreakout', 'news', 'AskReddit', 'idiotsincars']\n",
        "keywords = ['road accident', 'car crash', 'traffic accident', 'collision', 'hit and run']\n",
        "\n",
        "# Step 3: Function to search Reddit for posts\n",
        "def search_reddit(subreddit, keyword, limit=200):\n",
        "    posts = []\n",
        "    subreddit_instance = reddit.subreddit(subreddit)\n",
        "\n",
        "    # Search posts within the subreddit using the keyword\n",
        "    for submission in subreddit_instance.search(keyword, limit=limit):\n",
        "        posts.append({\n",
        "            'title': submission.title,\n",
        "            'score': submission.score,\n",
        "            'id': submission.id,\n",
        "            'url': submission.url,\n",
        "            'num_comments': submission.num_comments,\n",
        "            'created': submission.created_utc,\n",
        "            'body': submission.selftext,\n",
        "            'subreddit': submission.subreddit.display_name\n",
        "        })\n",
        "\n",
        "    return posts\n",
        "\n",
        "# Step 4: Collect data from multiple subreddits using keywords\n",
        "all_posts = []\n",
        "for subreddit in subreddits:\n",
        "    for keyword in keywords:\n",
        "        posts = search_reddit(subreddit, keyword, limit=200)  # Fetch posts with the limit\n",
        "        all_posts.extend(posts)  # Add posts to the main dataset\n",
        "        if len(all_posts) >= 1000:  # Stop collecting once we have 1000 posts\n",
        "            break\n",
        "    if len(all_posts) >= 1000:\n",
        "        break\n",
        "\n",
        "# Step 5: Convert the data into a pandas DataFrame\n",
        "df = pd.DataFrame(all_posts)\n",
        "\n",
        "# Step 6: Save the data into a CSV file for analysis\n",
        "df.to_csv('reddit_road_accidents_dataset.csv', index=False)\n",
        "\n",
        "print(f\"Collected {len(df)} posts related to road accidents.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63xntF-VZu1K",
        "outputId": "3193799d-b528-4b04-ccc4-77b2c14d7673"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collected 1068 posts related to road accidents.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03jb4GZsBkBS"
      },
      "source": [
        "## Question 3 (10 Points)\n",
        "Write Python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"XYZ\". The articles should be published in the last 10 years (2014-2024).\n",
        "\n",
        "The following information from the article needs to be collected:\n",
        "\n",
        "(1) Title of the article\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import requests\n",
        "\n",
        "# SerpAPI setup\n",
        "API_KEY = \"f4d269e8a21b0b0cbec8cb8c4f5b515769ca927a65ff549eb0d67a665e92855b\"\n",
        "SEARCH_ENGINE = \"google_scholar\"\n",
        "\n",
        "def search_google_scholar(query, num_results=1000):\n",
        "    \"\"\"Searches Google Scholar using SerpAPI and returns the articles.\"\"\"\n",
        "    params = {\n",
        "        \"engine\": SEARCH_ENGINE,\n",
        "        \"q\": query,\n",
        "        \"api_key\": API_KEY,\n",
        "        \"num\": num_results,  # Number of results to fetch\n",
        "        \"as_ylo\": \"2014\",    # Start year of publication\n",
        "        \"as_yhi\": \"2024\",    # End year of publication\n",
        "    }\n",
        "\n",
        "    response = requests.get(\"https://serpapi.com/search\", params=params)\n",
        "    data = response.json()\n",
        "    return data.get(\"organic_results\", [])\n",
        "\n",
        "def save_articles_to_csv(articles, filename=\"articles.csv\"):\n",
        "    \"\"\"Saves article information to a CSV file.\"\"\"\n",
        "    # Define CSV headers\n",
        "    headers = [\"Title\", \"Venue/Journal/Conference\", \"Year\", \"Authors\", \"Abstract\"]\n",
        "\n",
        "    # Open CSV file for writing\n",
        "    with open(filename, mode=\"w\", newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(headers)  # Write header row\n",
        "\n",
        "        # Write article rows\n",
        "        for article in articles:\n",
        "            title = article.get(\"title\", \"N/A\")\n",
        "            venue = article.get(\"publication_info\", {}).get(\"venue\", \"N/A\")\n",
        "            year = article.get(\"publication_info\", {}).get(\"year\", \"N/A\")\n",
        "            authors = \", \".join(article.get(\"authors\", []))\n",
        "            abstract = article.get(\"snippet\", \"N/A\")\n",
        "\n",
        "            writer.writerow([title, venue, year, authors, abstract])\n",
        "\n",
        "def main():\n",
        "    # Perform search on Google Scholar for keyword 'XYZ'\n",
        "    query = \"XYZ\"\n",
        "    articles = search_google_scholar(query)\n",
        "\n",
        "    # Save the results to CSV\n",
        "    save_articles_to_csv(articles)\n",
        "    print(f\"Saved {len(articles)} articles to 'articles.csv'.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5Qh8M5_CHqw",
        "outputId": "8bf3112d-ad1e-40e5-89b3-745d666b2099"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved 0 articles to 'articles.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJDe71iLB616"
      },
      "source": [
        "## Question 4A (10 Points)\n",
        "Develop Python code to collect data from social media platforms like Reddit, Instagram, Twitter (formerly known as X), Facebook, or any other. Use hashtags, keywords, usernames, or user IDs to gather the data.\n",
        "\n",
        "\n",
        "\n",
        "Ensure that the collected data has more than four columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install praw\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xceKz6tGfPso",
        "outputId": "045978e4-3e5a-49e7-c268-3d9504582334"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: praw in /usr/local/lib/python3.10/dist-packages (7.7.1)\n",
            "Requirement already satisfied: prawcore<3,>=2.1 in /usr/local/lib/python3.10/dist-packages (from praw) (2.4.0)\n",
            "Requirement already satisfied: update-checker>=0.18 in /usr/local/lib/python3.10/dist-packages (from praw) (0.18.0)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.10/dist-packages (from praw) (1.8.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from prawcore<3,>=2.1->praw) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import praw\n",
        "import pandas as pd\n",
        "\n",
        "# Step 1: Set up Reddit API connection using PRAW\n",
        "reddit = praw.Reddit(\n",
        "    client_id='F4rfCMsg7srHMauT3EauVA',  # Replace with your client ID\n",
        "    client_secret='Yj8hwZkknqi_xpWGvB6FgF3EGDLmzQ',  # Replace with your client secret\n",
        "    user_agent='Big_Jaguar_8911'  # Replace with your user agent\n",
        ")\n",
        "\n",
        "# Step 2: Function to search Reddit posts using keywords\n",
        "def search_reddit(keyword, limit=100):\n",
        "    posts = []\n",
        "\n",
        "    # Search Reddit for submissions containing the keyword\n",
        "    for submission in reddit.subreddit(\"all\").search(keyword, limit=limit):\n",
        "        posts.append({\n",
        "            'title': submission.title,\n",
        "            'username': submission.author.name if submission.author else 'N/A',\n",
        "            'post_id': submission.id,\n",
        "            'url': submission.url,\n",
        "            'num_comments': submission.num_comments,\n",
        "            'score': submission.score,\n",
        "            'created': submission.created_utc,\n",
        "            'body': submission.selftext,\n",
        "            'subreddit': submission.subreddit.display_name\n",
        "        })\n",
        "\n",
        "    return posts\n",
        "\n",
        "# Step 3: Define search terms and collect data\n",
        "keywords = ['road accident', 'car crash', 'traffic accident', 'collision']\n",
        "data = []\n",
        "\n",
        "# Collect data for each keyword\n",
        "for keyword in keywords:\n",
        "    keyword_data = search_reddit(keyword, limit=100)  # Fetch 100 posts per keyword\n",
        "    data.extend(keyword_data)\n",
        "\n",
        "# Step 4: Convert the collected data to a pandas DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Ensure that the dataset has at least four columns (we have more than four columns here)\n",
        "df = df[['title', 'username', 'post_id', 'url', 'num_comments', 'score', 'subreddit', 'created', 'body']]\n",
        "\n",
        "# Step 5: Save the collected data into a CSV file for further analysis\n",
        "df.to_csv('reddit_posts_data.csv', index=False)\n",
        "\n",
        "print(f\"Collected {len(df)} posts from Reddit related to the keywords: {', '.join(keywords)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3n6Mj7pfRUe",
        "outputId": "03e5a90f-86ab-4e86-bb52-b931510b9bfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collected 400 posts from Reddit related to the keywords: road accident, car crash, traffic accident, collision\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55W9AMdXCSpV"
      },
      "source": [
        "## Question 4B (10 Points)\n",
        "If you encounter challenges with Question-4 web scraping using Python, employ any online tools such as ParseHub or Octoparse for data extraction. Introduce the selected tool, outline the steps for web scraping, and showcase the final output in formats like CSV or Excel.\n",
        "\n",
        "\n",
        "\n",
        "Upload a document (Word or PDF File) in any shared storage (preferably UNT OneDrive) and add the publicly accessible link in the below code cell.\n",
        "\n",
        "Please only choose one option for question 4. If you do both options, we will grade only the first one"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZOhks1dXWEe"
      },
      "source": [
        "# Mandatory Question"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqmHVEwaWhbV"
      },
      "source": [
        "**Important: Reflective Feedback on Web Scraping and Data Collection**\n",
        "\n",
        "\n",
        "\n",
        "Please share your thoughts and feedback on the web scraping and data collection exercises you have completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on web scraping tasks. What were the key concepts or techniques you found most beneficial in understanding the process of extracting data from various online sources?\n",
        "\n",
        "\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in collecting data from certain websites, and how did you overcome them? If you opted for the non-coding option, share your experience with the chosen tool.\n",
        "\n",
        "\n",
        "\n",
        "Relevance to Your Field of Study: How might the ability to gather and analyze data from online sources enhance your work or research?\n",
        "\n",
        "**(no grading of your submission if this question is left unanswered)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akAVJn9YBTQT"
      },
      "source": [
        "Reflective Feedback on Web Scraping and Data Collection\n",
        "\n",
        "Learning Experience:\n",
        "Working on web scraping tasks helps in understanding the various ways to gather data from online sources, enhancing skills in Python programming and data extraction techniques. Key concepts like handling APIs, parsing HTML and managing data storage are crucial.\n",
        "\n",
        "Challenges Encountered:\n",
        "Challenges  include handling dynamic web content, dealing with rate limits and managing data consistency. For question 3, could not gain API key for any websites except for Google Scholar which implied restriction of only 100 searches for a month. Hence, could not achieve the required result in the file.\n",
        "\n",
        "Relevance to Your Field of Study:\n",
        "Mastering data collection from online sources can greatly enhance research capabilities, providing valuable insights and supporting data-driven decision-making in various fields."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "55W9AMdXCSpV"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}